---
title: "Coursera: Practical Machine Learning Course Project"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
vignette: >
  %\VignetteIndexEntry{project-writeup}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---
```{r knitrSetup, echo=FALSE, message=FALSE}
knitr::opts_knit$set(root.dir = "..")
```

```{r loadPkg, echo=FALSE,message=FALSE}
devtools::load_all()
data(pml_train)
data(pml_test)
```

## Loading the data

The following code loads the training and testing datasets into R:
```{r loadDataFromURL, eval=FALSE}
pml_train <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
pml_test <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```


## Splitting the data

We set a seed value to make the data partitioning reproducible.

```{r splitData, warning=FALSE, message=FALSE}
str(pml_train)

library(caret)

set.seed(126377)
train_idx <- createDataPartition(pml_train$classe, p = .6, list = FALSE)

training <- pml_train[train_idx, ]
testing <- pml_train[-train_idx, ]
```


## Exploratory Data Analysis


```{r edaFactors}
str(training[, sapply(training, is.factor)])
```

There are several factor variables in the dataset. However, it seems like some of them should really be numeric, since they have many levels. Some algorithms can not handle factors with more than 32 levels (e.g. `tree::tree()`). Hence, all factor variables with more than 32 levels are transformed into numerical variables. In addition, the `user_name` variable should be a character variable (although there are only 6 user names in the training set, the set of all possible user names might be large), while `cvtd_timestamp` should be a datetime format.

We create a function for factor transformation, so the same transformations can later be easily applied to the test dataset.

```{r transformFactors}
transformFactors <- function(data) {
  cols <- sapply(data, is.factor) & grepl("skewness|kurtosis|yaw", names(data))
  data[, cols] <- as.numeric(as.character(data[, cols]))
  data$user_name <- as.character(data$user_name)
  data$cvtd_timestamp <- lubridate::parse_date_time(data$cvtd_timestamp, "d/m/Y H:M")
  return(data)
}

training <- transformFactors(training)
str(training[, sapply(training, is.factor)])
```

In order to visualize the dataset in two dimensional space, we perform _Principal Component Analysis_ (PCA). Before PCA can be applied, we need to:

- Remove variables with zero variance (`caret::nearZeroVar` is used to identify such variables)
- Remove non-numerical variables
- Center and Scale variables

The first two principal components capture most of the variance in the dataset.

```{r edaPCA}
# identify zero variance variables
nzv <- nearZeroVar(training, saveMetrics = TRUE)
#remove zero variance and non-numeric variables
train_num <- training[, !nzv$zeroVar & sapply(training, is.numeric)]
# scale and center, then perform PCA
pca_train <- prcomp(train_num[complete.cases(train_num), -1],
                     center = TRUE,
                     scale. = TRUE)
plot(pca_train)
```

Next, we plot the first two principal components and use the `classe` variable to color the observations. The plot indicates that the observations fall into 3-4 clusters. However, those cluster do not separate the `classe` variable values very well.

```{r edaPCAPlot}
library(ggplot2)
library(scales)

idxs <- train_num[complete.cases(train_num), "X"]

ggplot(data.frame(pc1 = pca_train$x[, 1], 
                  pc2 = pca_train$x[, 2], 
                  classe = training[training$X %in% idxs, "classe"]), 
       aes(pc1, pc2)) + 
  geom_point(aes(color = classe), size = 3) +
  ggtitle("First 2 principal components of the WLE dataset")
```

## Data Preprocessing
We remove variables which

- have 0 variance
- have `NA` values in it
- identify individual rows (index variable `X`) or study participants (`user_name`), 
- are timestamps (`cvtd_timestamp`, `raw_timestamp_part_1`, `raw_timestamp_part_2`), 
- and time window indicators (`num_window`, `new_window`).

This gives us the `train_sub` dataset which will be used for model fitting.

```{r dataPrePoc}
train_sub <- training[, !nzv$zeroVar & 
                        !(names(training) %in% c("X", "user_name", "cvtd_timestamp",
                                                "new_window", "raw_timestamp_part_1",
                                                "raw_timestamp_part_2", "num_window"))]
train_sub <- train_sub[, !sapply(train_sub, function(x) any(is.na(x)))]
str(train_sub)
```


## Model Fitting

Random Forest is the model of choice. However, when training the model, we run into _performance issues_: The time requirement of the model fitting procedure for all of the `r nrow(train_sub)` observations in the training dataset is too high. Hence, we resample the train dataset to contain only about 1000 observations.

However, this gives us the chance to compare several classification algorithms on a (relatively large) validation dataset:

1. `train_small` contains 10% of the observations of the training set created above
2. `validation` contains 90% of the observations of the training set created above


```{r resamplingTrainingset}
set.seed(7824)
train_small_idx <- createDataPartition(train_sub$classe, p = .1, list = FALSE)
train_small <- train_sub[train_small_idx, ]
validation <- train_sub[-train_small_idx, ]
nrow(train_small)
```

_Cross Validation_ is applied to choose the model tuning parameters (`mtry`, i.e. the number of variables to consider on each split in the random forest model). The cross validated accuracy is a measure of the expected accuracy on new observations.

```{r modelFitting, message = FALSE}
set.seed(3377)
rf_fit <- train(classe ~ ., train_small, 
                method = "rf",
                tuneLength = 5,
                trControl = trainControl(method = "cv"))
rf_fit
plot(varImp(rf_fit))

set.seed(34671)
rpart_fit <- train(classe ~ ., train_small, 
                method = "rpart",
                tuneLength = 20,
                trControl = trainControl(method = "cv"))

(rf_acc <- confusionMatrix(predict(rf_fit, validation), validation$classe)$overall["Accuracy"])
(rpart_acc <- confusionMatrix(predict(rpart_fit, validation), validation$classe)$overall["Accuracy"])
```

The Random Forest performs much better on the validation set than the CART classification tree (`r round(rf_acc * 100, 2)`% vs. `r round(rpart_acc * 100, 2)`% accuracy) 

## Model Evaluation

The random forest is tested on the hold out observations in the testset. The accuracy on the testset is the final measure of accuracy on out of sample observations. The same preprocessing as for the trainingset has to be applied to the testset. The following preprocessing steps have been performed:

- transform factors
- drop variables

We do not need to drop variables on the testset explicitly. Instead, the model will simply ignore those variables when calculating predictions.

```{r testsetAccuracy, warning = FALSE}
testing <- transformFactors(testing)
confusionMatrix(predict(rf_fit, testing), testing$classe)
```

Predictions for the `pml_test` dataset (required for submission and automated scoring) can be obtained as follows:

```{r obtainPredictions, warning=FALSE}
pml_test <- transformFactors(pml_test)
submission <- predict(rf_fit, pml_test)
submission
```